{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b760a0f6",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "This example shows how to design a simple quantization aware training CNN and how to measure \n",
    "the maximum accumulator bit width. We also show the impact of quantization on accuracy. \n",
    "\n",
    "To analyze the potential FHE-compatible quantization configurations, we use the simulation mode of\n",
    " the FHE circuit of the CNN. The maximum accumulator \n",
    "bit width that is currently allowed in Concrete ML is **16** bits, which limits the bit-widths \n",
    "that are usable for weights and activations.\n",
    "\n",
    "We find an FHE compatible configuration for 3 bits weights and activations and run the CNN in FHE.\n",
    "The accuracy in this highly quantized configuration is 92%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253288cf",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn.utils import prune\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# And some helpers for visualization.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from dataset_utils import get_loaded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed920ba",
   "metadata": {},
   "source": [
    "### Load the data-set and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "humans_path = './data/human-and-non-human/training_set/training_set/humans'\n",
    "not_humans_path = './data/human-and-non-human/training_set/training_set/non-humans'\n",
    "IMAGE_SIZE = 64\n",
    "image_dimensions = (IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "transposed_image_dimensions = (1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "x_train, x_test, y_train, y_test = get_loaded_dataset(\n",
    "    humans_path,\n",
    "    not_humans_path,\n",
    "    IMAGE_SIZE\n",
    ")\n",
    "x_train[0].shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10587e3bfb3cb5da"
  },
  {
   "cell_type": "markdown",
   "id": "9c5e392d",
   "metadata": {},
   "source": [
    "### Define the neural network\n",
    "\n",
    "Since the accumulator bit width in FHE is small, we prune the convolutional\n",
    "filters to limit the number of connections per neuron.\n",
    "\n",
    "Neural network **pruning** is the process by which the synapses of individual neurons in a layer\n",
    "are forced to have a weight equal to zero. This basically eliminates them from the computation \n",
    "and thus they do not increase the accumulator bit width. It has been shown that neural networks can \n",
    "maintain their accuracy with a degree of pruning that can \n",
    "even exceed 70% for some over-parametrized networks such as VGG16 or large ResNets.\n",
    "\n",
    "See: https://arxiv.org/pdf/2003.03033.pdf, Figure 8 in Section 7.2, for an evaluation on the \n",
    "simple pruning method used in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.nn as qnn\n",
    "\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    \"\"\"A very small CNN to classify the sklearn digits data-set.\n",
    "\n",
    "    This class also allows pruning to a maximum of 10 active neurons, which\n",
    "    should help keep the accumulator bit width low.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, n_bits) -> None:\n",
    "        \"\"\"Construct the CNN with a configurable number of classes.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        a_bits = n_bits\n",
    "        w_bits = n_bits\n",
    "\n",
    "        # This network has a total complexity of 1216 MAC\n",
    "        # Quantization Layer\n",
    "        self.q1 = qnn.QuantIdentity(bit_width=a_bits, return_quant_tensor=True)\n",
    "\n",
    "        # Single Convolution\n",
    "        self.conv1 = qnn.QuantConv2d(1, 16, 5, stride=4, padding=2, weight_bit_width=w_bits)\n",
    "        self.q2 = qnn.QuantIdentity(bit_width=a_bits, return_quant_tensor=True)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = qnn.QuantLinear(16 * 16 * 16, 2, bias=True, weight_bit_width=w_bits)\n",
    "\n",
    "        # Enable pruning, prepared for training\n",
    "        # self.toggle_pruning(True)\n",
    "        \n",
    "        device = get_device()\n",
    "        self.to(device)\n",
    "        # Send all layers to the device\n",
    "        # self.q1.to(device)\n",
    "        # self.conv1.to(device)\n",
    "        # self.q2.to(device)\n",
    "        # self.fc1.to(device) \n",
    "\n",
    "\n",
    "    # def toggle_pruning(self, enable):\n",
    "    #     \"\"\"Enables or removes pruning.\"\"\"\n",
    "    # \n",
    "    #     # Maximum number of active neurons (i.e., corresponding weight != 0)\n",
    "    #     n_active = 12\n",
    "    # \n",
    "    #     # Go through all the convolution layers\n",
    "    #     for layer in [self.conv1]:\n",
    "    #         s = layer.weight.shape\n",
    "    # \n",
    "    #         # Compute fan-in (number of inputs to a neuron)\n",
    "    #         # and fan-out (number of neurons in the layer)\n",
    "    #         st = [s[0], np.prod(s[1:])]\n",
    "    # \n",
    "    #         # The number of input neurons (fan-in) is the product of\n",
    "    #         # the kernel width x height x inChannels.\n",
    "    #         if st[1] > n_active:\n",
    "    #             if enable:\n",
    "    #                 # This will create a forward hook to create a mask tensor that is multiplied\n",
    "    #                 # with the weights during forward. The mask will contain 0s or 1s\n",
    "    #                 prune.l1_unstructured(layer, \"weight\", (st[1] - n_active) * st[0])\n",
    "    #             else:\n",
    "    #                 # When disabling pruning, the mask is multiplied with the weights\n",
    "    #                 # and the result is stored in the weights member\n",
    "    #                 prune.remove(layer, \"weight\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run inference on the modified CNN, apply the decision layer on the reshaped conv output.\"\"\"\n",
    "        x = self.q1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.q2(x)\n",
    "\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO move to library\n",
    "def set_device_for_all_layers(model, device):\n",
    "    \"\"\"\n",
    "    Move all sub-modules of a model to the specified device.\n",
    "    \n",
    "    Args:\n",
    "    - model (torch.nn.Module): The PyTorch model\n",
    "    - device (torch.device): The target device (\"cuda\" or \"cpu\")\n",
    "    \"\"\"\n",
    "    for layer in model.children():\n",
    "        layer.to(device)\n",
    "\n",
    "def get_device():\n",
    "    # Check gpu availability\n",
    "    # if torch.cuda.is_available():\n",
    "    #     device = torch.device(\"cuda\")\n",
    "    #     print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    # else:\n",
    "    #     device = torch.device(\"cpu\")\n",
    "    #     print(\"Using CPU\")\n",
    "    # return device\n",
    "    return torch.device(\"cpu\")\n",
    "            "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae86caa94f0f5601"
  },
  {
   "cell_type": "markdown",
   "id": "a1449d54",
   "metadata": {},
   "source": [
    "### Train the CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "518771f3",
   "metadata": {},
   "source": [
    "Note that the training code for quantization aware training is the same as it would be for\n",
    "floating point training. Indeed, the Brevitas layers used in the CNN class will handle\n",
    "quantization during training. \n",
    "\n",
    "We train the network for varying weights and activations bit-width, to find an FHE compatible \n",
    "configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3035684",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def train_one_epoch(net, optimizer, train_loader):\n",
    "    # Cross Entropy loss for classification when not using a softmax layer in the network\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    net.train()\n",
    "    avg_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss_net = loss(output, target.long())\n",
    "        loss_net.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss_net.item()\n",
    "\n",
    "    return avg_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# Create the tiny CNN with 10 output classes\n",
    "N_EPOCHS = 150\n",
    "\n",
    "# Create a train data loader\n",
    "x_train_tensor = torch.Tensor(x_train).to(get_device())\n",
    "y_train_tensor = torch.Tensor(y_train).to(get_device())\n",
    "print(x_train_tensor.device)\n",
    "print(y_train_tensor.device)\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "# Create a test data loader to supply batches for network evaluation (test)\n",
    "x_test_tensor = torch.Tensor(x_test).to(get_device())\n",
    "y_test_tensor = torch.Tensor(y_test).to(get_device())\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "nets = []\n",
    "bit_range = range(2, 5)\n",
    "\n",
    "# Train the network with Adam, output the test set accuracy every epoch\n",
    "losses = []\n",
    "for n_bits in bit_range:\n",
    "    net = TinyCNN(2, n_bits)\n",
    "    losses_bits = []\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    for _ in tqdm(range(N_EPOCHS), desc=f\"Training with {n_bits} bit weights and activations\"):\n",
    "        losses_bits.append(train_one_epoch(net, optimizer, train_dataloader))\n",
    "    losses.append(losses_bits)\n",
    "\n",
    "    # Finally, disable pruning (sets the pruned weights to 0)\n",
    "    # net.toggle_pruning(False)\n",
    "    nets.append(net)\n",
    "    \n",
    "\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "for losses_bits in losses:\n",
    "    plt.plot(losses_bits)\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(list(map(str, bit_range)))\n",
    "plt.title(\"Training set loss during training\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fdc93",
   "metadata": {},
   "source": [
    "### Test the torch network in fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_torch(net, n_bits, test_loader):\n",
    "    \"\"\"Test the network: measure accuracy on the test set.\"\"\"\n",
    "\n",
    "    # Freeze normalization layers\n",
    "    net.eval()\n",
    "\n",
    "    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "\n",
    "    # Iterate over the batches\n",
    "    idx = 0\n",
    "    for data, target in test_loader:\n",
    "        # Accumulate the ground truth labels\n",
    "        endidx = idx + target.shape[0]\n",
    "        all_targets[idx:endidx] = target.numpy()\n",
    "\n",
    "        # Run forward and get the predicted class id\n",
    "        output = net(data).argmax(1).detach().numpy()\n",
    "        all_y_pred[idx:endidx] = output\n",
    "\n",
    "        idx += target.shape[0]\n",
    "\n",
    "    # Print out the accuracy as a percentage\n",
    "    n_correct = np.sum(all_targets == all_y_pred)\n",
    "    print(\n",
    "        f\"Test accuracy for {n_bits}-bit weights and activations: \"\n",
    "        f\"{n_correct / len(test_loader) * 100:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "for idx, net in enumerate(nets):\n",
    "    test_torch(net, bit_range[idx], test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14bc3f",
   "metadata": {},
   "source": [
    "### Define the Concrete ML testing function\n",
    "\n",
    "We introduce the `test_with_concrete` function which allows us to test a Concrete ML model in one of two modes:\n",
    "- in FHE\n",
    "- in the clear, using simulated FHE execution\n",
    "\n",
    "Note that it is trivial to toggle between between the two modes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_with_concrete(quantized_module, test_loader, use_sim):\n",
    "    \"\"\"Test a neural network that is quantized and compiled with Concrete ML.\"\"\"\n",
    "\n",
    "    # Casting the inputs into int64 is recommended\n",
    "    all_y_pred = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "    all_targets = np.zeros((len(test_loader)), dtype=np.int64)\n",
    "\n",
    "    # Iterate over the test batches and accumulate predictions and ground truth labels in a vector\n",
    "    idx = 0\n",
    "    for data, target in tqdm(test_loader):\n",
    "        data = data.numpy()\n",
    "        target = target.numpy()\n",
    "\n",
    "        fhe_mode = \"simulate\" if use_sim else \"execute\"\n",
    "\n",
    "        # Quantize the inputs and cast to appropriate data type\n",
    "        y_pred = quantized_module.forward(data, fhe=fhe_mode)\n",
    "\n",
    "        endidx = idx + target.shape[0]\n",
    "\n",
    "        # Accumulate the ground truth labels\n",
    "        all_targets[idx:endidx] = target\n",
    "\n",
    "        # Get the predicted class id and accumulate the predictions\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        all_y_pred[idx:endidx] = y_pred\n",
    "\n",
    "        # Update the index\n",
    "        idx += target.shape[0]\n",
    "\n",
    "    # Compute and report results\n",
    "    n_correct = np.sum(all_targets == all_y_pred)\n",
    "    print(all_targets)\n",
    "    print(all_y_pred)\n",
    "\n",
    "    return n_correct / len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cccf0b5",
   "metadata": {},
   "source": [
    "### Test the network using Simulation\n",
    "\n",
    "Note that this is not a test in FHE. The simulated FHE mode gives \n",
    "insight into the number of accumulator bits that are needed and the \n",
    "impact of FHE execution on the accuracy.\n",
    "\n",
    "The torch/brevitas neural network is quantized during training and, for inference, it is converted \n",
    "to FHE by Concrete ML using a dedicated function, `compile_brevitas_qat_model`.\n",
    "\n",
    "In this test we determine the accuracy and accumulator bit-widths for the various quantization settings\n",
    "that are trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33250d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "accum_bits = []\n",
    "sim_time = []\n",
    "\n",
    "\n",
    "for idx in range(len(bit_range)):\n",
    "    q_module = compile_brevitas_qat_model(nets[idx], x_train)\n",
    "\n",
    "    accum_bits.append(q_module.fhe_circuit.graph.maximum_integer_bit_width())\n",
    "\n",
    "    start_time = time.time()\n",
    "    accs.append(\n",
    "        test_with_concrete(\n",
    "            q_module,\n",
    "            test_dataloader,\n",
    "            use_sim=True,\n",
    "        )\n",
    "    )\n",
    "    sim_time.append(time.time() - start_time)\n",
    "\n",
    "for idx, vl_time_bits in enumerate(sim_time):\n",
    "    print(\n",
    "        f\"Simulated FHE execution for {bit_range[idx]} bit network: {vl_time_bits:.2f}s, \"\n",
    "        f\"{len(test_dataloader) / vl_time_bits:.2f}it/s\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4666bf",
   "metadata": {},
   "source": [
    "### Analysis of quantized results\n",
    "\n",
    "We plot the accuracies obtained for various levels of quantization of weights and activations. \n",
    "In addition, we plot the maximum accumulator bit width required to run inference of the network for\n",
    "each weight and activation bit width. This is shown as the numbers next to the graph markers. \n",
    "\n",
    "This accumulator bit width is determined by the compiler and is an important quantity in designing FHE-compatible neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "plt.plot(bit_range, accs, \"-x\")\n",
    "for bits, acc, accum in zip(bit_range, accs, accum_bits):\n",
    "    plt.gca().annotate(str(accum), (bits - 0.1, acc + 0.01))\n",
    "plt.ylabel(\"Accuracy on test set\")\n",
    "plt.xlabel(\"Weight & activation quantization\")\n",
    "plt.grid(True)\n",
    "plt.title(\n",
    "    \"Accuracy for varying quantization bit width. Accumulator bit-width shown on graph markers\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb5a3f",
   "metadata": {},
   "source": [
    "### Test the CNN in FHE\n",
    "\n",
    "We identify 3 bit weights and activations as a good compromise for which the maximum accumulator size\n",
    "is low but the accuracy is acceptable. We can now compile to FHE and execute on encrypted data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218aff7",
   "metadata": {},
   "source": [
    "### 1. Compile to FHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa5e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_for_fhe = 3\n",
    "idx_bits_fhe = bit_range.index(bits_for_fhe)\n",
    "\n",
    "accum_bits_required = accum_bits[idx_bits_fhe]\n",
    "\n",
    "q_module_fhe = None\n",
    "\n",
    "net = nets[idx_bits_fhe]\n",
    "\n",
    "q_module_fhe = compile_brevitas_qat_model(\n",
    "    net,\n",
    "    x_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2875e825",
   "metadata": {},
   "source": [
    "### 2. Generate Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keys first, this may take some time (up to 30min)\n",
    "t = time.time()\n",
    "q_module_fhe.fhe_circuit.keygen()\n",
    "print(f\"Keygen time: {time.time()-t:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd257f",
   "metadata": {},
   "source": [
    "### 3. Execute in FHE on encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_image(image_data, filename):\n",
    "    # print(image_data.shape)\n",
    "    # Create an Image object from the NumPy array\n",
    "    image = Image.fromarray(image_data.squeeze().astype(\"uint8\"), mode='L')\n",
    "    # print(image.size)\n",
    "    # Save the image to a file\n",
    "    image.save(filename)  # Change 'saved_image.jpg' to your desired file name and format\n",
    "\n",
    "def transpose_data_for_image(data):\n",
    "    # Transpose the data to match the image format\n",
    "    data = data.transpose(1, 2, 0)\n",
    "    # Convert the data to uint8\n",
    "    data = data.astype(\"uint8\")\n",
    "    # print(data.shape)\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31e604ffe50467b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run inference in FHE on a single encrypted example\n",
    "index = 20\n",
    "mini_test_dataset = TensorDataset(torch.Tensor(x_test[[index], :]), torch.Tensor(y_test[[index]]))\n",
    "mini_test_dataloader = DataLoader(mini_test_dataset)\n",
    "# Save x_text[0] as an image\n",
    "print(x_test[0].shape)\n",
    "print(int(y_test[index]))\n",
    "save_image(transpose_data_for_image(x_test[index]), \"saved_image.jpg\")\n",
    "print(len(x_test))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dd474ad3a361bef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"./data/franquito.jpg\"\n",
    "# #Open image\n",
    "img = Image.open(path).resize(image_dimensions[:2]).convert('L')\n",
    "img_array = np.array(img)\n",
    "\n",
    "test_set = np.zeros((1, *transposed_image_dimensions), dtype='float32')\n",
    "test_set[0] = img_array\n",
    "print(test_set[[0]].shape)\n",
    "\n",
    "save_image(transpose_data_for_image(test_set[0]), \"saved_image.jpg\")\n",
    "\n",
    "mini_test_dataset = TensorDataset(torch.Tensor(test_set[[0]]), torch.Tensor([[1]]))\n",
    "mini_test_dataloader = DataLoader(mini_test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab6b0dfa7a4721c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "res = test_with_concrete(\n",
    "    q_module_fhe,\n",
    "    mini_test_dataloader,\n",
    "    use_sim=False,\n",
    ")\n",
    "print(f\"Time per inference in FHE: {(time.time() - t) / len(mini_test_dataset):.2f}\")\n",
    "print(f\"Accuracy in FHE: {res:.2f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dea752e80679b34"
  },
  {
   "cell_type": "markdown",
   "id": "550f48bf",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We see that quantization with **3** bit weight and activations is the best viable FHE configuration,\n",
    "as the accumulator bit width for this configuration is between **7 and 8** bits (can vary due to the final \n",
    "distribution of the weights). The accuracy in this setting, 92% is a few percentage points \n",
    "under the maximum accuracy achievable with larger accumulator bit widths (97-98%). \n",
    "\n",
    "Compiling the higher bit-width networks is also possible, but in this example, to ensure FHE execution is fast\n",
    "we used the lower bit-width quantization setting.\n"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
